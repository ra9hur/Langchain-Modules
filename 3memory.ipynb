{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413840, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334328, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fdf357cd5d47058a9d3334ee842227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf' # Model path for Llama-2 finetuned chat model\n",
    "\n",
    "# tokenizer creation\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "# importing pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             load_in_4bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map='auto',\n",
    "                max_new_tokens = 512,\n",
    "                do_sample = True,\n",
    "                top_k=1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe,\n",
    "                          model_kwargs = {'temperature' : 0.7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2 chat prompt creation\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "# default system prompt\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, \n",
    "while being safe. Your answers should not include any harmful, unethical, racist, sexist, \n",
    "toxic, dangerous, or illegal content. Please ensure that your responses are socially \n",
    "unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
    "answering something not correct. If you don't know the answer to a question, please don't \n",
    "share false information.\"\"\"\n",
    "\n",
    "\n",
    "# Function to create prompt templete with user provided instruction and with system prompt / default system prompt \n",
    "\n",
    "def get_template(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_chain(instruction,system_prompt=DEFAULT_SYSTEM_PROMPT,format_instructions=None):\n",
    "    \"\"\"\"\n",
    "    get_llm_chain does below mentioned series of steps:\n",
    "    \n",
    "      1. Take instruction and system prompt to create the templete.\n",
    "      2. Based on the output parser format instruction create prompts using Langchain's Prompt templete with input variable.\n",
    "      3. Create the llm using previously created Hugging Face pipeline \n",
    "      4. Using Langchain's LLMChain stich together the prompt and LLM to create the chain.\n",
    "      \n",
    "    \"\"\"\n",
    "    template = get_templete(instruction, system_prompt)\n",
    "    if format_instructions:\n",
    "        prompt = PromptTemplate(template=template, \n",
    "                                input_variables=[\"text\"] , \n",
    "                                partial_variables={\"format_instructions\": format_instructions})\n",
    "    else:\n",
    "        prompt = PromptTemplate(template=template, \n",
    "                                input_variables=[\"text\"])\n",
    "    \n",
    "    print('\\n Prompt Templete: \\n \\n',template)\n",
    "    llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    return llm_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Unterscheidung zwischen \"Cricket World Cup\" und \"ICC Cricket World Cup\". The ICC Cricket World Cup is the premier international tournament of men\\'s cricket, held every four years. It is organized by the International Cricket Council (ICC) and features the top teams from around the world competing in a round-robin format. The tournament is considered one of the most prestigious in international cricket and is watched by millions of fans around the globe.\\n\\nThe most recent edition of the ICC Cricket World Cup took place in England and Wales in 2019. The tournament was won by England, who defeated New Zealand in the final by 119 runs. This was England\\'s first World Cup title, and it marked a significant moment in the country\\'s cricketing history.\\n\\nIn contrast, the Cricket World Cup is a broader term that can refer to any international cricket tournament, including those organized by the ICC as well as those held by other governing bodies or organizations. The term \"Cricket World Cup\" is often used more generally to refer to any major international cricket tournament, regardless of the specific organizer or format.\\n\\nIn summary, the ICC Cricket World Cup is a specific international cricket tournament organized by the ICC every four years, featuring the top teams from around the world competing in a round-robin format. The most recent edition of the tournament was won by England in 2019. The term \"Cricket World Cup\" can refer to any major international cricket tournament, including those organized by other governing bodies or organizations.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the set-up\n",
    "\n",
    "instruction = \"Can you please let me know, Who won the last {text} world cup? \\n\"\n",
    "\n",
    "llm_chain = get_llm_chain(instruction)\n",
    "\n",
    "output = llm_chain.run('football')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_conversation = ConversationChain(llm=llm,\n",
    "                                      memory=memory,\n",
    "                                      verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_template('What do you know about AI')\n",
    "\n",
    "chat_conversation.predict(input=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_template('What do you know about Llama2?')\n",
    "\n",
    "chat_conversation.predict(input=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickled = pickle.dumps(chat_conversation.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('memory.pkl','wb') as f:\n",
    "    f.write(pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_load = open('memory.pkl','rb').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_conversation = ConversationChain(llm=llm,\n",
    "                                     memory = pickle.loads(memory_load),\n",
    "                                     verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory_1 = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "chat_conversation_window = ConversationChain(llm=llm, \n",
    "                                             memory=memory_1, \n",
    "                                             verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_template('Hello, How are you doing today?')\n",
    "\n",
    "chat_conversation_window.predict(input=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_template('What is the date today?')\n",
    "\n",
    "chat_conversation_window.predict(input=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_1.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory_summ = ConversationSummaryBufferMemory(llm=llm,\n",
    "                                              max_token_limit=100)\n",
    "\n",
    "chat_conversation_summ = ConversationChain(llm=llm,\n",
    "                                           memory=memory_summ,\n",
    "                                           verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_conversation_summ.predict(input='What do you know about AI?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_conversation_summ.predict(input='What do you know about Machine Learning?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_summ.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
