{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever vector store size is big, it can be very hard for the end user to frame the correct query string, to retrieve the correct similarity search\n",
    "\n",
    "Instead of worrying about generating the correct phrasing of query, we can use multiquery vector which can generate multiple versions of the query and this can in turn be provided to llm or the vector store and it can give you back better results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_paper_path = './LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load documents and Retriever Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LangChain pdf document loader\n",
    "from langchain.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and create pages\n",
    "loader = PyPDFLoader(file_path=llama2_paper_path)\n",
    "pages = loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(file_path=llama2_paper_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(documents,\n",
    "                           embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "# Default system prompt\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, \n",
    "while being safe. Your answers should not include any harmful, unethical, racist, sexist, \n",
    "toxic, dangerous, or illegal content. Please ensure that your responses are socially \n",
    "unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
    "answering something not correct. If you don't know the answer to a question, please don't \n",
    "share false information.\n",
    "\n",
    "Always say \"thanks for asking!\" at the end of the answer. \"\"\"\n",
    "\n",
    "def get_prompt_template(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
    "    System_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    PromptTemplate = B_INST + System_PROMPT + instruction + E_INST\n",
    "\n",
    "    return PromptTemplate\n",
    "\n",
    "instruction = '''Use the following pieces of context to answer the question at the end. \n",
    "{context}\n",
    "Question: {question}\\n' \n",
    "Helpful Answer:'''\n",
    "\n",
    "template = get_prompt_template(instruction)\n",
    "\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM and HF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import GPTQConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "mname = \"TheBloke/Mistral-7B-OpenOrca-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quantization_config_loading = GPTQConfig(bits=4, \n",
    "                                         disable_exllama=True, \n",
    "                                         use_cuda_fp16=True,\n",
    "                                         tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(mname,\n",
    "                                             quantization_config=quantization_config_loading,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 256,\n",
    "                do_sample=True,\n",
    "                top_k=1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2\n",
    "                )\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "# Set logging for the queries\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muilti-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "mq_retriever = MultiQueryRetriever.from_llm(llm=llm,\n",
    "                                            retriever=db.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"what is so special about llama 2?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:LLMChain > 3:llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: what is so special about llama 2?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghu/anaconda3/envs/autogptq/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What makes Llama 2 unique or notable in comparison to other models?', '    Why has Llama 2 gained attention and popularity among researchers and users alike?', '    How does Llama 2 differ from previous language models in terms of its capabilities and performance?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:LLMChain > 3:llm:HuggingFacePipeline] [71.88s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\n    \\n    What makes Llama 2 unique or notable in comparison to other models?\\n    Why has Llama 2 gained attention and popularity among researchers and users alike?\\n    How does Llama 2 differ from previous language models in terms of its capabilities and performance?\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:LLMChain] [71.88s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "query = \"what is so special about llama 2?\"\n",
    "\n",
    "output = mq_retriever.get_relevant_documents(query=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='els that have not been ﬁnetuned on code, namely\\nPaLM and LaMDA (Thoppilan et al., 2022). PaLM\\nand LLaMA were trained on datasets that contain\\na similar number of code tokens.\\nAs show in Table 8, for a similar number\\nof parameters, LLaMA outperforms other gen-\\neral models such as LaMDA and PaLM, which\\nare not trained or ﬁnetuned speciﬁcally for code.\\nLLaMA with 13B parameters and more outper-\\nforms LaMDA 137B on both HumanEval and\\nMBPP. LLaMA 65B also outperforms PaLM 62B,', metadata={'page': 5, 'source': './LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'}),\n",
       " Document(page_content='LLaMA-13B outperforms GPT-3 on most bench-\\nmarks, despite being 10 ×smaller. We believe that\\nthis model will help democratize the access and\\nstudy of LLMs, since it can be run on a single GPU.\\nAt the higher-end of the scale, our 65B-parameter\\nmodel is also competitive with the best large lan-\\nguage models such as Chinchilla or PaLM-540B.\\nUnlike Chinchilla, PaLM, or GPT-3, we only\\nuse publicly available data, making our work com-\\npatible with open-sourcing, while most existing', metadata={'page': 0, 'source': './LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'}),\n",
       " Document(page_content='to train an instruct model, LLaMA-I.\\nOPT 30B 26.1\\nGLM 120B 44.8\\nPaLM 62B 55.1\\nPaLM-cont 62B 62.8\\nChinchilla 70B 67.5\\nLLaMA 65B 63.4\\nOPT-IML-Max 30B 43.2\\nFlan-T5-XXL 11B 55.1\\nFlan-PaLM 62B 59.6\\nFlan-PaLM-cont 62B 66.1\\nLLaMA-I 65B 68.9\\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\\nComparison of models of moderate size with and with-\\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\\nmodel LLaMA-I on MMLU and compare with ex-', metadata={'page': 6, 'source': './LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'}),\n",
       " Document(page_content='et al., 2022). On GSM8k, we observe that LLaMA-\\n65B outperforms Minerva-62B, although it has not\\nbeen ﬁne-tuned on mathematical data.\\n3.5 Code generation\\nWe evaluate the ability of our models to write\\ncode from a natural language description on two\\nbenchmarks: HumanEval (Chen et al., 2021) and\\nMBPP (Austin et al., 2021). For both tasks, the\\nmodel receives a description of the program in a\\nfew sentences, as well as a few input-output ex-\\namples. In HumanEval, it also receives a function', metadata={'page': 4, 'source': './LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'}),\n",
       " Document(page_content='D Generations from LLaMA-I\\nWe show a few examples of generations with LLaMA-I, i.e. LLaMA-65B ﬁne-tuned with the protocol\\nand instruction dataset from Chung et al. (2022).\\nwrite a conversation between the sun and pluto\\nSun: Hey Pluto, what’s up?\\nPluto: Not much. Just hanging out here at the edge of the solar system.\\nSun: Yeah, I’ve been meaning to talk to you about that. You’re not a planet anymore.\\nPluto: What? Come on, man. That’s not fair.', metadata={'page': 21, 'source': './LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'}),\n",
       " Document(page_content='2019), and to generate toxic or offensive con-\\ntent (Gehman et al., 2020). As our training dataset\\ncontains a large proportion of data from the Web,\\nwe believe that it is crucial to determine the po-\\ntential for our models to generate such content.\\nTo understand the potential harm of LLaMA-65B,\\nwe evaluate on different benchmarks that measure\\ntoxic content production and stereotypes detection.\\nWhile we have selected some of the standard bench-\\nmarks that are used by the language model com-', metadata={'page': 6, 'source': './LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'}),\n",
       " Document(page_content='We compare LLaMA with other foundation mod-\\nels, namely the non-publicly available language\\nmodels GPT-3 (Brown et al., 2020), Gopher (Rae\\net al., 2021), Chinchilla (Hoffmann et al., 2022)\\nand PaLM (Chowdhery et al., 2022), as well as\\nthe open-sourced OPT models (Zhang et al., 2022),\\nGPT-J (Wang and Komatsuzaki, 2021), and GPT-\\nNeo (Black et al., 2022). In Section 4, we also\\nbrieﬂy compare LLaMA with instruction-tuned\\nmodels such as OPT-IML (Iyer et al., 2022) and', metadata={'page': 3, 'source': './LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'}),\n",
       " Document(page_content='ing on the abilities of large language models.\\n8 Conclusion\\nIn this paper, we presented a series of language\\nmodels that are released openly, and competitive\\nwith state-of-the-art foundation models. Most\\nnotably, LLaMA-13B outperforms GPT-3 while\\nbeing more than 10 ×smaller, and LLaMA-65B is\\ncompetitive with Chinchilla-70B and PaLM-540B.\\nUnlike previous studies, we show that it is possible\\nto achieve state-of-the-art performance by training\\nexclusively on publicly available data, without', metadata={'page': 10, 'source': './LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
