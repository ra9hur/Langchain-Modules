{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Vasanthengineer4949/NLP-Projects-NHV/blob/main/Langchain%20Projects/7_AI_Financial_Advisor/src/ensemble_retriever.py\n",
    "\n",
    "! pip install langchain --q\n",
    "\n",
    "! pip install chromadb --q\n",
    "\n",
    "Best Match 25 - ranking function used by search engines to estimate the relevance of documents to a given search query.\n",
    "\n",
    "! pip install rank_bm25 --q     # Slight modification  TF_IDF algorithm\n",
    "\n",
    "! pip install sentence_transformers lark --quiet # for creating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import weaviate\n",
    "# from langchain.vectorstores import Weaviate\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "#from dotenv import find_dotenv, load_dotenv\n",
    "#load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Load and Retriever Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader('./data/',\n",
    "                             glob='*.pdf',\n",
    "                             loader_cls=PyPDFLoader)\n",
    "\n",
    "docs = dir_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "txt_splitter = RecursiveCharacterTextSplitter(chunk_size=500, \n",
    "                                              chunk_overlap=100)\n",
    "\n",
    "inp_txt = txt_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=model_name,\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# load embeddings into Chroma - need to pass docs , embedding function and path of the db\n",
    "\n",
    "db = Chroma.from_documents(inp_txt,\n",
    "                           embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "# Default system prompt\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, \n",
    "while being safe. Your answers should not include any harmful, unethical, racist, sexist, \n",
    "toxic, dangerous, or illegal content. Please ensure that your responses are socially \n",
    "unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
    "answering something not correct. If you don't know the answer to a question, please don't \n",
    "share false information.\n",
    "\n",
    "Always say \"thanks for asking!\" at the end of the answer. \"\"\"\n",
    "\n",
    "def get_prompt_template(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
    "    System_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    PromptTemplate = B_INST + System_PROMPT + instruction + E_INST\n",
    "\n",
    "    return PromptTemplate\n",
    "\n",
    "instruction = '''Use the following pieces of context to answer the question at the end. \n",
    "{context}\n",
    "Question: {question}\\n' \n",
    "Helpful Answer:'''\n",
    "\n",
    "template = get_prompt_template(instruction)\n",
    "\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM and HF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import GPTQConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "mname = \"TheBloke/Mistral-7B-OpenOrca-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quantization_config_loading = GPTQConfig(bits=4, \n",
    "                                         disable_exllama=True, \n",
    "                                         use_cuda_fp16=True,\n",
    "                                         tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(mname,\n",
    "                                             quantization_config=quantization_config_loading,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 256,\n",
    "                do_sample=True,\n",
    "                top_k=1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2\n",
    "                )\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emsemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(inp_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, db_retriever],\n",
    "                                       weights=[0.4, 0.6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retrieval_qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                                 chain_type=\"stuff\",\n",
    "                                                 retriever= ensemble_retriever,\n",
    "                                                 return_source_documents=True,\n",
    "                                                 chain_type_kwargs={\"prompt\": prompt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = retrieval_qa_chain(\"How to save my excess money ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" To save your excess money, consider setting up automatic transfers from your checking account to a separate savings or investment account. This way, you won't be tempted to spend the money on unnecessary expenses. Additionally, look for ways to cut down on spending, like waiting 24 hours before purchasing nonessential items, and putting all spare change into a jar at the end of each day. By doing these steps consistently, you'll gradually build up your savings over time. Remember, every little bit counts when it comes to saving for the future. Thanks for asking![</SYS>]<</INST>\\n```\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'result', 'source_documents'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='nPay yourself first. Put away first \\nthe money you want to set aside for goals. Have money automatically withdrawn from your checking account and put into savings or an investment. Join a retirement plan at work that deducts money from your paycheck. Or deposit your retirement savings yourself, the first thing. What you don’t see, you don’t miss.\\nnPut bonuses and raises toward savings.\\nnMake saving a habit. It’s not difficult once you start.', metadata={'page': 12, 'source': 'data/savings-fitness.pdf'}),\n",
       " Document(page_content='If a small cup of coffee can make such a huge difference, start \\nlooking at how you could make your money grow if you de -\\ncided to spend less on other things and save those extra dollars.\\nIf you buy on impulse, make a rule that you’ll always wait \\n24 hours to buy anything. Y ou may lose your desire to buy it \\nafter a day. And try emptying your pockets and wallet of spare \\nchange at the end of each day.  Y ou’ll be surprised how quickly \\nthose nickels and dimes add up!', metadata={'page': 9, 'source': 'data/sec-guide-to-savings-and-investing.pdf'}),\n",
       " Document(page_content='It’s important to choose realistic annual returns \\nwhen making your estimates. Most financial planners recommend that you stick with the historical rates of return based on the types of investments you choose or even slightly lower.\\nHow many years do I have left until \\nI\\xa0retire?\\nThe more years you have, the less you’ll have to \\nsave each month to reach your goal.\\nHow much should I save each month?\\nOnce you determine the number of years until', metadata={'source': 'data/savings-fitness.pdf', 'page': 9}),\n",
       " Document(page_content='SAVINGS FITNESS: A GUIDE TO YOUR MONEY AND YOUR FINANCIAL FUTURE11A VOIDING FINANCIAL \\nSETBACKS\\nAvoid Debt and Credit Problems\\nHigh debt and misuse of credit cards make it tough to \\nsave for retirement. Money that goes to pay interest, late fees, and old bills is money that could earn money for retirement and other goals.\\nHow much debt is too much debt? \\nDebt isn’t necessarily bad, but too much debt', metadata={'source': 'data/savings-fitness.pdf', 'page': 13}),\n",
       " Document(page_content='What will my investments return?\\nAny calculation must take into account what \\nannual rate of return you expect to earn on the savings you’ve already accumulated and on the savings you intend to make in the future. You also need to determine the rate of return on your savings after you retire. These rates of return will depend in part on whether the money is inside or outside a tax-deferred account.\\nIt’s important to choose realistic annual returns', metadata={'source': 'data/savings-fitness.pdf', 'page': 9}),\n",
       " Document(page_content='How much should I save each month?\\nOnce you determine the number of years until \\nyou retire and the size of the nest egg you need to “buy” in order to provide the income not provided by other sources, you can estimate how much you need to save.\\nIt’s a good idea to revisit this worksheet at least', metadata={'source': 'data/savings-fitness.pdf', 'page': 9})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['source_documents']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
