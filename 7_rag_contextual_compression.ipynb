{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual compression works well with LLaMA2 quantized model\n",
    "\n",
    "Provides summarized version of page content instead of full page content which gets a bit difficult to comprehend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time\n",
    "import transformers # HF import\n",
    "from langchain import HuggingFacePipeline # To build the HF pipeline using Llama-2\n",
    "from langchain import PromptTemplate,  LLMChain # To create PromptTemplate and LLMChain\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM , AutoModel  # For creating the model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTQConfig\n",
    "\n",
    "#mname = 'TheBloke/Llama-2-7B-Chat-GGUF'\n",
    "mname = \"TheBloke/Mistral-7B-OpenOrca-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quantization_config_loading = GPTQConfig(bits=4, \n",
    "                                         disable_exllama=True, \n",
    "                                         use_cuda_fp16=True,\n",
    "                                         tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(mname,\n",
    "                                             quantization_config=quantization_config_loading,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 128,\n",
    "                do_sample=True,\n",
    "                top_k=1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2\n",
    "                )\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Preparation\n",
    "\n",
    "Source : https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. This significantly reduces overfitting and gives major improvements over other regularization methods\",\n",
    "        metadata={\"name\":\"Dropout: a simple way to prevent neural networks from overfitting\", \"year\": 2014, \n",
    "                  \"Authors\": \"Hinton, G.E., Krizhevsky, A., Srivastava, N., Sutskever, I., & Salakhutdinov, R.\", \"cited\":2084 , \n",
    "                  \"Field\" : \"'Neural Network','Regularization'\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"We present a residual learning framework to ease the training of deep neural networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.\",\n",
    "        metadata={\"name\":\"Deep Residual Learning for Image Recognition\", \"year\": 2016, \n",
    "                  \"Authors\": \"He, K., Ren, S., Sun, J., & Zhang, X. (2016). CoRR\", \"cited\":1436 , \n",
    "                  \"Field\" : \"'Image Recognition','Computer Vision'\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change.  We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.  Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin.\",\n",
    "        metadata={\"name\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", \"year\": 2015, \n",
    "                  \"Authors\": \"Sergey Ioffe, Christian Szegedy\", \"cited\":946 , \n",
    "                  \"Field\" : \"'Neural Network','Deep Learning','Speed up Training Process'\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes\",\n",
    "        metadata={\"name\":\"Large-Scale Video Classification with Convolutional Neural Networks \", \"year\": 2014, \n",
    "                  \"Authors\": \"Fei-Fei, L., Karpathy, A., Leung, T., Shetty, S., Sukthankar, R., & Toderici, G.\", \"cited\":865 , \n",
    "                  \"Field\" : \"'Convolutional Neural Network','Deep Learning','Video Classfication'\"},\n",
    "    ),\n",
    "    Document(\n",
    "       page_content=\"We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.\",\n",
    "       metadata={\"name\":\"Microsoft COCO: Common Objects in Context\", \"year\": 2014, \n",
    "                  \"Authors\": \"Belongie, S.J., Dollár, P., Hays, J., Lin, T., Maire, M., Perona, P., Ramanan, D., & Zitnick, C.L\", \"cited\":830 , \n",
    "                 \"Field\" : \"'Convolutional Neural Network','Object Detection','Dataset'\"},\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(docs, embeddings) ### Vector Store Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, \n",
    "                                                       base_retriever=db.as_retriever())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing simple Similarity Search based on a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search('Please let me know the details about batch normalization ?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " Document(page_content=\"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change.  We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.  Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin.\", metadata={'Authors': 'Sergey Ioffe, Christian Szegedy', 'Field': \"'Neural Network','Deep Learning','Speed up Training Process'\", 'cited': 946, 'name': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'year': 2015}))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs), docs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using compression_retriever to get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compression_retriever.get_relevant_documents(\"Please let me know the details about Dropout?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\"randomly drop units\", \"preventing units from co-adapting too much\", \"reduces overfitting\"', metadata={'Authors': 'Hinton, G.E., Krizhevsky, A., Srivastava, N., Sutskever, I., & Salakhutdinov, R.', 'Field': \"'Neural Network','Regularization'\", 'cited': 2084, 'name': 'Dropout: a simple way to prevent neural networks from overfitting', 'year': 2014}),\n",
       " Document(page_content='Residual Learning Framework, Easier Training, Deep Neural Networks', metadata={'Authors': 'He, K., Ren, S., Sun, J., & Zhang, X. (2016). CoRR', 'Field': \"'Image Recognition','Computer Vision'\", 'cited': 1436, 'name': 'Deep Residual Learning for Image Recognition', 'year': 2016}),\n",
       " Document(page_content='None\\n```', metadata={'Authors': 'Fei-Fei, L., Karpathy, A., Leung, T., Shetty, S., Sukthankar, R., & Toderici, G.', 'Field': \"'Convolutional Neural Network','Deep Learning','Video Classfication'\", 'cited': 865, 'name': 'Large-Scale Video Classification with Convolutional Neural Networks ', 'year': 2014}),\n",
       " Document(page_content='- Training Deep Neural Networks\\n- Internal covariate shift\\n- Batch Normalization\\n- State-of-the-art image classification model\\n- Fewer training steps\\n- Significant margin\\n```', metadata={'Authors': 'Sergey Ioffe, Christian Szegedy', 'Field': \"'Neural Network','Deep Learning','Speed up Training Process'\", 'cited': 946, 'name': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'year': 2015})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"randomly drop units\", \"preventing units from co-adapting too much\", \"reduces overfitting\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Residual Learning Framework, Easier Training, Deep Neural Networks'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1].page_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Contextual Compression on Larger Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LangChain pdf document loader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_of_llms = 'survey_of_large_lang_models.pdf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and create pages\n",
    "loader = PyPDFLoader(file_path=survey_of_llms)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the embeddings to the existing Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = db.add_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search('Please let me know how the Large Language Models are trained?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs ### Huge content the whole page content is retrieved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying the Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghu/anaconda3/envs/autogptq/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/raghu/anaconda3/envs/autogptq/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/raghu/anaconda3/envs/autogptq/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/home/raghu/anaconda3/envs/autogptq/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = compression_retriever.get_relevant_documents(\"Please let me know how the Large Language Models are trained?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121] A. Radford, R. J ´ozefowicz, and I. Sutskever, “Learning to generate reviews and discovering sentiment,” CoRR, vol. abs/1704.01444, 2017.\n",
      "[122] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improving language understanding by generative pre-training,” 2018.\n",
      "---------------\n",
      "- Large Language Models (LLM): Pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks.\n",
      "- Model Scaling: Researchers have found that model scaling can lead to an improved model capacity. When the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., in-context learning) that are not present in small-scale language models (e.g., B\n",
      "---------------\n",
      "[30] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown,\n",
      "B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\n",
      "D. Amodei, “Scaling laws for neural language mod-\n",
      "els,” CoRR, vol. abs/2001.08361, 2020.\n",
      "[31] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph,\n",
      "---------------\n",
      "Large Language Models (LLMs) are trained using various techniques such as pre-processing, tokenization, masked language modeling, next sentence prediction, denoising autoencoders, generative pretrained transformer objectives, and others. The training process involves collecting massive amounts of text data, processing it into input sequences for the model, optimizing the model's parameters through backpropagation, and evaluating its performance on downstream tasks.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for i in results:\n",
    "    print(i.page_content)\n",
    "    print('---------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
