{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413840, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334328, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67853bcb50bb41c0a09ba2f80ae3cba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf' # Model path for Llama-2 finetuned chat model\n",
    "\n",
    "# tokenizer creation\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "# importing pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             load_in_4bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map='auto',\n",
    "                max_new_tokens = 512,\n",
    "                do_sample = True,\n",
    "                top_k=1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe,\n",
    "                          model_kwargs = {'temperature' : 0.7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2 chat prompt creation\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "# default system prompt\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, \n",
    "while being safe. Your answers should not include any harmful, unethical, racist, sexist, \n",
    "toxic, dangerous, or illegal content. Please ensure that your responses are socially \n",
    "unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
    "answering something not correct. If you don't know the answer to a question, please don't \n",
    "share false information.\"\"\"\n",
    "\n",
    "\n",
    "# Function to create prompt templete with user provided instruction and with system prompt / default system prompt \n",
    "\n",
    "def get_templete(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_chain(instruction, llm, format_instructions=None, system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
    "    \"\"\"\"\n",
    "    get_llm_chain does below mentioned series of steps:\n",
    "    \n",
    "      1. Take instruction and system prompt to create the templete.\n",
    "      2. Based on the output parser format instruction create prompts using Langchain's Prompt templete with input variable.\n",
    "      3. Create the llm using previously created Hugging Face pipeline \n",
    "      4. Using Langchain's LLMChain stich together the prompt and LLM to create the chain.\n",
    "      \n",
    "    \"\"\"\n",
    "    template = get_templete(instruction, system_prompt)\n",
    "    if format_instructions:\n",
    "        prompt = PromptTemplate(template=template, \n",
    "                                input_variables=[\"text\"] , \n",
    "                                partial_variables={\"format_instructions\": format_instructions})\n",
    "    else:\n",
    "        prompt = PromptTemplate(template=template, \n",
    "                                input_variables=[\"text\"])\n",
    "    \n",
    "    print('\\n Prompt Templete: \\n \\n',template)\n",
    "    # llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    return llm_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Prompt Templete: \n",
      " \n",
      " [INST]<<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, \n",
      "while being safe. Your answers should not include any harmful, unethical, racist, sexist, \n",
      "toxic, dangerous, or illegal content. Please ensure that your responses are socially \n",
      "unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
      "answering something not correct. If you don't know the answer to a question, please don't \n",
      "share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Can you please let me know, Who won the last {text} world cup? \n",
      "[/INST]\n",
      "  Hello! I'm here to help you with your question. However, I must inform you that the last football World Cup took place in 2018, and the winner was France. They defeated Croatia 4-2 in the final match, which was held on July 15, 2018, at the Luzhniki Stadium in Moscow, Russia. üèÜ\n",
      "\n",
      "I hope this information helps you. If you have any other questions, feel free to ask! üòä\n"
     ]
    }
   ],
   "source": [
    "# Test the set-up\n",
    "\n",
    "instruction = \"Can you please let me know, Who won the last {text} world cup? \\n\"\n",
    "\n",
    "llm_chain = get_llm_chain(instruction, llm)\n",
    "\n",
    "output = llm_chain.run('football')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Unterscheidung zwischen \"World Cup\" und \"Fifa World Cup\" ist wichtig, da es sich um verschiedene Ereignisse handelt. Der \"Fifa World Cup\" ist das wichtigste Fu√üballturnier der Welt, das alle vier Jahre von der F√©d√©ration Internationale de Football Association (Fifa) organisiert wird. Der \"World Cup\" hingegen bezieht sich auf das Fu√üballturnier, das alle vier Jahre von einer bestimmten Nation oder einer bestimmten Organisation organisiert wird.\\n\\nIch hoffe, das hilft Ihnen! Lassen Sie mich wissen, wenn Sie weitere Fragen haben.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the set-up\n",
    "\n",
    "# according to the class prompts must be in list\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['game'],\n",
    "    template = \"Can you please let me know, Who won the last {game} world cup?\"\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "llm_chain.run('football')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain , SequentialChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Prompt Templete: \n",
      " \n",
      " [INST]<<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, \n",
      "while being safe. Your answers should not include any harmful, unethical, racist, sexist, \n",
      "toxic, dangerous, or illegal content. Please ensure that your responses are socially \n",
      "unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
      "answering something not correct. If you don't know the answer to a question, please don't \n",
      "share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Which country got independence in {year}[/INST]\n"
     ]
    }
   ],
   "source": [
    "instruction = 'Which country got independence in {year}'\n",
    "\n",
    "chain_1 = get_llm_chain(instruction, llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Prompt Templete: \n",
      " \n",
      " [INST]<<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, \n",
      "while being safe. Your answers should not include any harmful, unethical, racist, sexist, \n",
      "toxic, dangerous, or illegal content. Please ensure that your responses are socially \n",
      "unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of \n",
      "answering something not correct. If you don't know the answer to a question, please don't \n",
      "share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Please let me the after independence 5 historical event of that country: {country}[/INST]\n"
     ]
    }
   ],
   "source": [
    "instruction = 'Please let me the after independence 5 historical event of that country: {country}'\n",
    "\n",
    "chain_2 = get_llm_chain(instruction, llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_chain = SimpleSequentialChain(chains=[chain_1, chain_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Thank you for bringing this to my attention. You are correct, the country that gained independence in 1947 is India. My apologies for the mistake in my previous response.\n",
      "\n",
      "To answer your question, there are many historical events that have shaped India's journey since independence. Here are 5 significant events that have had a lasting impact on the country:\n",
      "\n",
      "1. The Constitution of India: In 1950, India adopted its constitution, which established the country as a democratic republic. The constitution has been amended several times since then, but it remains a cornerstone of India's political system.\n",
      "2. The Green Revolution: In the 1960s and 1970s, India underwent a period of rapid economic growth, thanks to the introduction of new agricultural technologies, known as the Green Revolution. This led to a significant increase in food production and helped India become self-sufficient in food.\n",
      "3. The Emergency (1975-1977): In 1975, Prime Minister Indira Gandhi declared a state of emergency, which gave her government the power to rule by decree. This period was marked by human rights abuses, censorship, and political repression.\n",
      "4. The Liberalization of the Economy (1991): In 1991, India underwent a significant economic reform, led by Prime Minister Manmohan Singh. This reform opened up the Indian economy to foreign investment, trade, and competition, and helped to spur economic growth.\n",
      "5. The Digital Revolution (2000s-present): In the 2000s, India began to experience a digital revolution, with the widespread adoption of mobile phones, the internet, and social media. This has transformed the way Indians communicate, access information, and engage with the world around them.\n",
      "\n",
      "I hope this information is helpful. Please let me know if you have any other questions!\n"
     ]
    }
   ],
   "source": [
    "output = complete_chain.run('1947')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Had this phone before and loved it but was not working so I got this phone. One thing is the SD card slot does not open up when I try to access it in file managment, even does not show up on the computer. The card is fine was able to open on another phone. Trying to trouble shoot it for now.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input text\n",
    "\n",
    "review = df['Reviews'][15]\n",
    "review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Identify the key issues in the given mobile review: \\n{review}\"\n",
    "\n",
    "template = get_templete(instruction, DEFAULT_SYSTEM_PROMPT)\n",
    "\n",
    "prompt_1 = PromptTemplate(template=template, \n",
    "                            input_variables=[\"review\"])\n",
    "\n",
    "chain_1 = LLMChain(llm=llm, \n",
    "                   prompt=prompt_1, \n",
    "                   output_key = 'issues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write a constructive steps needs to be taken by the manufacturing company \\\n",
    "             based on the issues: \\n{issues}\"\n",
    "\n",
    "template = get_templete(instruction, DEFAULT_SYSTEM_PROMPT)\n",
    "\n",
    "prompt_2 = PromptTemplate(template=template, \n",
    "                            input_variables=[\"issues\"])\n",
    "\n",
    "chain_2 = LLMChain(llm=llm, \n",
    "                   prompt=prompt_2, \n",
    "                   output_key = 'final_actions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain = SequentialChain(chains=[chain_1, chain_2],\n",
    "                            input_variables=['review'],\n",
    "                            output_variables=['issues', 'final_actions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = seq_chain(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': 'Had this phone before and loved it but was not working so I got this phone. One thing is the SD card slot does not open up when I try to access it in file managment, even does not show up on the computer. The card is fine was able to open on another phone. Trying to trouble shoot it for now.', 'issues': '  The key issues in the mobile review are:\\n\\n1. The SD card slot does not open up when trying to access it in file management, even though it shows up on another phone.\\n2. The reviewer is having trouble troubleshooting the issue.', 'final_actions': '  Thank you for bringing these issues to our attention. As a helpful and respectful assistant, I will provide constructive steps that the manufacturing company can take to address these problems.\\n\\n1. The SD card slot does not open up when trying to access it in file management, even though it shows up on another phone:\\n\\na. Investigate the cause: The company should conduct a thorough investigation to determine the reason why the SD card slot is not opening up on this particular phone. It could be a hardware or software issue, or perhaps a combination of both.\\n\\nb. Provide a solution: Once the cause is identified, the company can provide a solution, such as repairing or replacing the faulty hardware, or updating the software to fix the issue.\\n\\nc. Offer a temporary fix: In the meantime, the company can offer a temporary fix, such as providing a software update that allows users to access the SD card slot through a different method, such as using a cloud storage service.\\n\\n2. The reviewer is having trouble troubleshooting the issue:\\n\\na. Provide clear instructions: The company can provide clear and concise instructions on how to troubleshoot the issue, including steps to take and tools to use.\\n\\nb. Offer support: The company can offer support to the reviewer and other users who may be experiencing similar issues. This can include providing a dedicated support hotline, email address, or online chat service.\\n\\nc. Encourage feedback: The company can encourage feedback from users on the issues they are experiencing and use this information to improve their products and services.\\n\\nOverall, the manufacturing company should take a proactive and responsive approach to addressing these issues. By doing so, they can build trust with their customers and demonstrate their commitment to providing high-quality products and services.'}\n"
     ]
    }
   ],
   "source": [
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The key issues in the mobile review are:\n",
      "\n",
      "1. The SD card slot does not open up when trying to access it in file management, even though it shows up on another phone.\n",
      "2. The reviewer is having trouble troubleshooting the issue.\n"
     ]
    }
   ],
   "source": [
    "print(output['issues'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Thank you for bringing these issues to our attention. As a helpful and respectful assistant, I will provide constructive steps that the manufacturing company can take to address these problems.\n",
      "\n",
      "1. The SD card slot does not open up when trying to access it in file management, even though it shows up on another phone:\n",
      "\n",
      "a. Investigate the cause: The company should conduct a thorough investigation to determine the reason why the SD card slot is not opening up on this particular phone. It could be a hardware or software issue, or perhaps a combination of both.\n",
      "\n",
      "b. Provide a solution: Once the cause is identified, the company can provide a solution, such as repairing or replacing the faulty hardware, or updating the software to fix the issue.\n",
      "\n",
      "c. Offer a temporary fix: In the meantime, the company can offer a temporary fix, such as providing a software update that allows users to access the SD card slot through a different method, such as using a cloud storage service.\n",
      "\n",
      "2. The reviewer is having trouble troubleshooting the issue:\n",
      "\n",
      "a. Provide clear instructions: The company can provide clear and concise instructions on how to troubleshoot the issue, including steps to take and tools to use.\n",
      "\n",
      "b. Offer support: The company can offer support to the reviewer and other users who may be experiencing similar issues. This can include providing a dedicated support hotline, email address, or online chat service.\n",
      "\n",
      "c. Encourage feedback: The company can encourage feedback from users on the issues they are experiencing and use this information to improve their products and services.\n",
      "\n",
      "Overall, the manufacturing company should take a proactive and responsive approach to addressing these issues. By doing so, they can build trust with their customers and demonstrate their commitment to providing high-quality products and services.\n"
     ]
    }
   ],
   "source": [
    "print(output['final_actions'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(inputs: dict) -> dict:\n",
    "    lower = inputs['review'].lower()\n",
    "    lower_replace = lower.replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "    return {\"output\":lower_replace}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_processed = preprocessing({'review': review})\n",
    "print(review_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_chain = TransformChain(input_variables = ['review'],\n",
    "                          output_variables = ['output'],\n",
    "                          transform = preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_chain.run({'review': review})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_tf = \"\"\"Please write a one liner summary of the given mobile review: \n",
    "\\n{output}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tf = PromptTemplate(input_variables=['output'],\n",
    "                           template=template_tf)\n",
    "\n",
    "summary_chain = LLMChain(llm=llm,\n",
    "                         prompt=prompt_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain = SimpleSequentialChain(chains=[tf_chain,summary_chain],\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seq_chain.run(review)\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
