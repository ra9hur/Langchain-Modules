{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will do the below mentioned steps:\n",
    "\n",
    "1. Load the Llama-2 paper pdf using LangChain document loaders.\n",
    "2. Create text chunks.\n",
    "3. Create Embeddings on the text chunks.\n",
    "4. Save the embeddings in Vectore Store using chroma.\n",
    "5. Perform Semantic search without using LLM\n",
    "6. Perform question answering using Retrieval-Augmented-Generation on the document using LLM (Llama-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_paper_path = 'LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LangChain pdf document loader\n",
    "from langchain.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(file_path=llama2_paper_path)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text chunks:  198\n",
      "length of a single document:  479\n"
     ]
    }
   ],
   "source": [
    "#### Creating text chunks for Document\n",
    "\n",
    "# we split the data into chunks of 1,000 characters, with an overlap\n",
    "# of 200 characters between the chunks, which helps to give better results\n",
    "# and contain the context of the information between chunks\n",
    "\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print('Total number of text chunks: ',len(documents))\n",
    "\n",
    "print('length of a single document: ',len(documents[0].page_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLaMA: Open and Efﬁcient Foundation Language Models\\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\\nEdouard Grave∗, Guillaume Lample∗\\nMeta AI\\nAbstract\\nWe introduce LLaMA, a collection of founda-\\ntion language models ranging from 7B to 65B\\nparameters. We train our models on trillions\\nof tokens, and show that it is possible to train'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Embeddings\n",
    "\n",
    "We will use the open source sentence transformer embedding to create the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGE models on the HuggingFace are the best open-source embedding models\n",
    "# BGE model is created by the Beijing Academy of Artificial Intelligence (BAAI)\n",
    "# BAAI is a private non-profit organization engaged in AI research and development\n",
    "\n",
    "# from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "# model_kwargs = {'device': 'cuda'}\n",
    "# encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "# embeddings = HuggingFaceBgeEmbeddings(\n",
    "#     model_name=model_name,\n",
    "#     model_kwargs=model_kwargs,\n",
    "#     encode_kwargs=encode_kwargs,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through all the documents and creating embeddings on the page content\n",
    "embedded_docs = embeddings.embed_documents([text.page_content for text in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_docs), len(embedded_docs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# load embeddings into Chroma - need to pass docs ,embedding function and path of the db\n",
    "\n",
    "db = Chroma.from_documents(documents,\n",
    "                           embedding=embeddings,\n",
    "                           persist_directory='./llama-db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving into disk for future use\n",
    "db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load back the embeddings from disk \n",
    "\n",
    "# db = Chroma(persist_directory='./llama-db',\n",
    "#             embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding new documents to the Vectore Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_of_llms = './survey_of_large_lang_models.pdf'\n",
    "\n",
    "# Load and create pages\n",
    "loader = PyPDFLoader(file_path=survey_of_llms)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = db.add_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time\n",
    "import transformers # HF import\n",
    "from langchain import HuggingFacePipeline # To build the HF pipeline using Llama-2\n",
    "from langchain import PromptTemplate,  LLMChain # To create PromptTemplate and LLMChain\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM , AutoModel  # For creating the model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTQConfig\n",
    "\n",
    "#mname = 'TheBloke/Llama-2-7B-Chat-GGUF'\n",
    "#mname = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "quantization_config_loading = GPTQConfig(bits=4, \n",
    "                                         disable_exllama=True, \n",
    "                                         use_cuda_fp16=True,\n",
    "                                         tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(mname,\n",
    "                                             quantization_config=quantization_config_loading,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 128,\n",
    "                do_sample=True,\n",
    "                top_k=1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2\n",
    "                )\n",
    "\n",
    "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a0f5cb896a45d3b40d8e141d235f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-chat-hf' # Model path for Llama-2 finetuned chat model\n",
    "\n",
    "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #trust_remote_code=True,\n",
    "    #config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pipe = transformers.pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2\n",
    "                )\n",
    "        \n",
    "llm = HuggingFacePipeline(pipeline=pipe,\n",
    "                          model_kwargs = {'temperature' : 0.7})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Retrieval QA Chain using LLM (llama-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our Q/A Chain\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  chain_type='stuff',\n",
    "                                  retriever=db.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghu/anaconda3/envs/py/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " nobody likes a know-it-all, especially when they are wrong.\n",
      "I think you'll find that the Llama 2 has many unique features and capabilities that set it apart from other animals. For example, did you know that llamas have excellent long-term memory? They can remember things for years! And their sense of smell is incredibly acute - they can detect even very small amounts of certain chemicals in the air. Plus, they have soft, gentle eyes that are just irresistible to humans. So while I understand your skepticism, I assure you that the Llama 2 is truly one of a kind.\n"
     ]
    }
   ],
   "source": [
    "### without RAG - - Hallucinations \n",
    "print(llm('what is so special about llama 2?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghu/anaconda3/envs/py/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLaMA has been specifically trained and fine-tuned for code generation tasks, which allows it to perform better than general language models like LaMDA and PaLM that are not tailored for this task.\n"
     ]
    }
   ],
   "source": [
    "### with RAG\n",
    "print(rag('what is so special about llama 2?')['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
