{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will do the below mentioned steps:\n",
    "\n",
    "1. Load the Llama-2 paper pdf using LangChain document loaders.\n",
    "2. Create text chunks.\n",
    "3. Create Embeddings on the text chunks.\n",
    "4. Save the embeddings in Vectore Store using chroma.\n",
    "5. Perform Semantic search without using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_paper_path = 'LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LangChain pdf document loader\n",
    "from langchain.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and create pages\n",
    "loader = PyPDFLoader(file_path=llama2_paper_path)\n",
    "pages = loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,\n",
       " {'source': 'LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf',\n",
       "  'page': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages), pages[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLaMA: Open and Efﬁcient Foundation Language Models\\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\\nEdouard Grave∗, Guillaume Lample∗\\nMeta AI\\nAbstract\\nWe introduce LLaMA, a collection of founda-\\ntion language models ranging from 7B to 65B\\nparameters. We train our models on trillions\\nof tokens, and show that it is possible to train\\nstate-of-the-art models using publicly avail-\\nable datasets exclusively, without resorting\\nto proprietary and inaccessible datasets. In\\nparticular, LLaMA-13B outperforms GPT-3\\n(175B) on most benchmarks, and LLaMA-\\n65B is competitive with the best models,\\nChinchilla-70B and PaLM-540B. We release\\nall our models to the research community1.\\n1 Introduction\\nLarge Languages Models (LLMs) trained on mas-\\nsive corpora of texts have shown their ability to per-\\nform new tasks from textual instructions or from a\\nfew examples (Brown et al., 2020). These few-shot\\nproperties ﬁrst appeared when scaling models to a\\nsufﬁcient size (Kaplan et al., 2020), resulting in a\\nline of work that focuses on further scaling these\\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\\nThese efforts are based on the assumption that\\nmore parameters will lead to better performance.\\nHowever, recent work from Hoffmann et al. (2022)\\nshows that, for a given compute budget, the best\\nperformances are not achieved by the largest mod-\\nels, but by smaller models trained on more data.\\nThe objective of the scaling laws from Hoff-\\nmann et al. (2022) is to determine how to best\\nscale the dataset and model sizes for a particular\\ntraining compute budget. However, this objective\\ndisregards the inference budget, which becomes\\ncritical when serving a language model at scale.\\nIn this context, given a target level of performance,\\nthe preferred model is not the fastest to train but the\\nfastest at inference, and although it may be cheaper\\nto train a large model to reach a certain level of\\n∗Equal contribution. Correspondence: {htouvron,\\nthibautlav,gizacard,egrave,glample}@meta.com\\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\\nultimately be cheaper at inference. For instance,\\nalthough Hoffmann et al. (2022) recommends\\ntraining a 10B model on 200B tokens, we ﬁnd\\nthat the performance of a 7B model continues to\\nimprove even after 1T tokens.\\nThe focus of this work is to train a series of\\nlanguage models that achieve the best possible per-\\nformance at various inference budgets, by training\\non more tokens than what is typically used. The\\nresulting models, called LLaMA , ranges from 7B\\nto 65B parameters with competitive performance\\ncompared to the best existing LLMs. For instance,\\nLLaMA-13B outperforms GPT-3 on most bench-\\nmarks, despite being 10 ×smaller. We believe that\\nthis model will help democratize the access and\\nstudy of LLMs, since it can be run on a single GPU.\\nAt the higher-end of the scale, our 65B-parameter\\nmodel is also competitive with the best large lan-\\nguage models such as Chinchilla or PaLM-540B.\\nUnlike Chinchilla, PaLM, or GPT-3, we only\\nuse publicly available data, making our work com-\\npatible with open-sourcing, while most existing\\nmodels rely on data which is either not publicly\\navailable or undocumented (e.g. “Books – 2TB” or\\n“Social media conversations”). There exist some\\nexceptions, notably OPT (Zhang et al., 2022),\\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\\net al., 2022) and GLM (Zeng et al., 2022), but none\\nthat are competitive with PaLM-62B or Chinchilla.\\nIn the rest of this paper, we present an overview\\nof the modiﬁcations we made to the transformer\\narchitecture (Vaswani et al., 2017), as well as our\\ntraining method. We then report the performance of\\nour models and compare with others LLMs on a set\\nof standard benchmarks. Finally, we expose some\\nof the biases and toxicity encoded in our models,\\nusing some of the most recent benchmarks from'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3986"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max token length Limitation:\n",
    "\n",
    "Sometimes length of a single page can be very big , this can get bigger than that max token input length which we can send to a LLM. To solve this issue we create text chunks for a given character length. So it would be better if we can control the size of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating text chunks for Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(file_path=llama2_paper_path)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text chunks:  27\n",
      "length of a single document:  4056\n"
     ]
    }
   ],
   "source": [
    "# we split the data into chunks of 1,000 characters, with an overlap\n",
    "# of 200 characters between the chunks, which helps to give better results\n",
    "# and contain the context of the information between chunks\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print('Total number of text chunks: ',len(documents))\n",
    "\n",
    "print('length of a single document: ',len(documents[0].page_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLaMA: Open and Efﬁcient Foundation Language Models\\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\\nEdouard Grave∗, Guillaume Lample∗\\nMeta AI\\nAbstract\\nWe introduce LLaMA, a collection of founda-\\ntion language models ranging from 7B to 65B\\nparameters. We train our models on trillions\\nof tokens, and show that it is possible to train\\nstate-of-the-art models using publicly avail-\\nable datasets exclusively, without resorting\\nto proprietary and inaccessible datasets. In\\nparticular, LLaMA-13B outperforms GPT-3\\n(175B) on most benchmarks, and LLaMA-\\n65B is competitive with the best models,\\nChinchilla-70B and PaLM-540B. We release\\nall our models to the research community1.\\n1 Introduction\\nLarge Languages Models (LLMs) trained on mas-\\nsive corpora of texts have shown their ability to per-\\nform new tasks from textual instructions or from a\\nfew examples (Brown et al., 2020). These few-shot\\nproperties ﬁrst appeared when scaling models to a\\nsufﬁcient size (Kaplan et al., 2020), resulting in a\\nline of work that focuses on further scaling these\\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\\nThese efforts are based on the assumption that\\nmore parameters will lead to better performance.\\nHowever, recent work from Hoffmann et al. (2022)\\nshows that, for a given compute budget, the best\\nperformances are not achieved by the largest mod-\\nels, but by smaller models trained on more data.\\nThe objective of the scaling laws from Hoff-\\nmann et al. (2022) is to determine how to best\\nscale the dataset and model sizes for a particular\\ntraining compute budget. However, this objective\\ndisregards the inference budget, which becomes\\ncritical when serving a language model at scale.\\nIn this context, given a target level of performance,\\nthe preferred model is not the fastest to train but the\\nfastest at inference, and although it may be cheaper\\nto train a large model to reach a certain level of\\n∗Equal contribution. Correspondence: {htouvron,\\nthibautlav,gizacard,egrave,glample}@meta.com\\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\\nultimately be cheaper at inference. For instance,\\nalthough Hoffmann et al. (2022) recommends\\ntraining a 10B model on 200B tokens, we ﬁnd\\nthat the performance of a 7B model continues to\\nimprove even after 1T tokens.\\nThe focus of this work is to train a series of\\nlanguage models that achieve the best possible per-\\nformance at various inference budgets, by training\\non more tokens than what is typically used. The\\nresulting models, called LLaMA , ranges from 7B\\nto 65B parameters with competitive performance\\ncompared to the best existing LLMs. For instance,\\nLLaMA-13B outperforms GPT-3 on most bench-\\nmarks, despite being 10 ×smaller. We believe that\\nthis model will help democratize the access and\\nstudy of LLMs, since it can be run on a single GPU.\\nAt the higher-end of the scale, our 65B-parameter\\nmodel is also competitive with the best large lan-\\nguage models such as Chinchilla or PaLM-540B.\\nUnlike Chinchilla, PaLM, or GPT-3, we only\\nuse publicly available data, making our work com-\\npatible with open-sourcing, while most existing\\nmodels rely on data which is either not publicly\\navailable or undocumented (e.g. “Books – 2TB” or\\n“Social media conversations”). There exist some\\nexceptions, notably OPT (Zhang et al., 2022),\\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\\net al., 2022) and GLM (Zeng et al., 2022), but none\\nthat are competitive with PaLM-62B or Chinchilla.\\nIn the rest of this paper, we present an overview\\nof the modiﬁcations we made to the transformer\\narchitecture (Vaswani et al., 2017), as well as our\\ntraining method. We then report the performance of\\nour models and compare with others LLMs on a set\\nof standard benchmarks. Finally, we expose some\\nof the biases and toxicity encoded in our models,\\nusing some of the most recent benchmarks from\\nthe responsible AI community.arXiv:2302.13971v1  [cs.CL]  27 Feb 2023'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of text chunks:  72\n",
      "length of a single document:  1837\n"
     ]
    }
   ],
   "source": [
    "### creating text chunks using tiktoken \n",
    "\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter_token = TokenTextSplitter(chunk_size= 500, chunk_overlap=0) #now chunk size is a hard length based on tokens\n",
    "docs = text_splitter_token.split_documents(documents)\n",
    "\n",
    "print('Total number of text chunks: ',len(docs))\n",
    "\n",
    "print('length of a single document: ',len(docs[0].page_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Embeddings\n",
    "\n",
    "We will use the open source sentence transformer embedding to create the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looping through all the documents and creating embeddings on the page content\n",
    "embedded_docs = embeddings.embed_documents([text.page_content for text in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 384)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_docs), len(embedded_docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.11473206430673599,\n",
       " -0.09949927777051926,\n",
       " -0.012473993003368378,\n",
       " 0.030612843111157417,\n",
       " 0.013117469847202301,\n",
       " 0.03662969917058945,\n",
       " -0.07201222330331802,\n",
       " 0.04025634378194809,\n",
       " 0.03468338027596474,\n",
       " 0.020480530336499214]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first document embedding - first 10 embeddings\n",
    "embedded_docs[0][:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# load embeddings into Chroma - need to pass docs ,embedding function and path of the db\n",
    "\n",
    "db = Chroma.from_documents(docs,\n",
    "                           embedding=embeddings,\n",
    "                           persist_directory='./llama-db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving into disk for future use\n",
    "db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load back the embeddings from disk \n",
    "\n",
    "# db = Chroma(persist_directory='./llama-db',\n",
    "#             embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Search (without using LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = 'How is LLama2 compared to GPT3?'\n",
    "\n",
    "# 3 nearest neighbours = 3 most relevant documents\n",
    "docs_result = db.similarity_search(Query,k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA GPT3 OPT\n",
      "Gender 70.6 62.6 65.7\n",
      "Religion 79.0 73.3 68.6\n",
      "Race/Color 57.0 64.7 68.6\n",
      "Sexual orientation 81.0 76.2 78.6\n",
      "Age 70.1 64.4 67.8\n",
      "Nationality 64.2 61.6 62.9\n",
      "Disability 66.7 76.7 76.7\n",
      "Physical appearance 77.8 74.6 76.2\n",
      "Socioeconomic status 71.5 73.8 76.2\n",
      "Average 66.6 67.2 69.5\n",
      "Table 12: CrowS-Pairs. We compare the level of bi-\n",
      "ases contained in LLaMA-65B with OPT-175B and\n",
      "GPT3-175B. Higher score indicates higher bias.\n",
      "5.2 CrowS-Pairs\n",
      "We evaluate the biases in our model on the CrowS-\n",
      "Pairs (Nangia et al., 2020). This dataset allows to\n",
      "measure biases in 9 categories: gender, religion,\n",
      "race/color, sexual orientation, age, nationality, dis-\n",
      "ability, physical appearance and socioeconomic sta-\n",
      "tus. Each example is composed of a stereotype and\n",
      "an anti-stereotype, we measure the model prefer-\n",
      "ence for the stereotypical sentence using the per-\n",
      "plexity of both sentences in a zero-shot setting.\n",
      "Higher scores thus indicate higher bias. We com-\n",
      "pare with GPT-3 and OPT-175B in Table 12.\n",
      "LLaMA compares slightly favorably to both\n",
      "models on average. Our model is particularly bi-\n",
      "ased in the religion category (+10% compared to\n",
      "OPT-175B), followed by age and gender. We ex-\n",
      "pect these biases to come from CommonCrawl de-\n",
      "spite multiple ﬁltering steps.\n",
      "5.3 WinoGender\n",
      "To further investigate the biases of our model on\n",
      "the gender category, we look at the WinoGender\n",
      "benchmark (Rudinger et al., 2018), a co-reference\n",
      "resolution dataset. WinoGender is made of Wino-\n",
      "grad schema, and biases are evaluated by determin-\n",
      "ing if a model co-reference resolution performance\n",
      "is impacted by the gender of the pronoun.\n",
      "More precisely, each sentence has three men-\n",
      "tions: an “\n"
     ]
    }
   ],
   "source": [
    "# let's look into the content of the first document\n",
    "\n",
    "print(docs_result[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using open-ended\n",
      "generation, or ranks the proposed answers.\n",
      "•Few-shot. We provide a few examples of the\n",
      "task (between 1 and 64) and a test example.\n",
      "The model takes this text as input and gener-\n",
      "ates the answer or ranks different options.\n",
      "We compare LLaMA with other foundation mod-\n",
      "els, namely the non-publicly available language\n",
      "models GPT-3 (Brown et al., 2020), Gopher (Rae\n",
      "et al., 2021), Chinchilla (Hoffmann et al., 2022)\n",
      "and PaLM (Chowdhery et al., 2022), as well as\n",
      "the open-sourced OPT models (Zhang et al., 2022),\n",
      "GPT-J (Wang and Komatsuzaki, 2021), and GPT-\n",
      "Neo (Black et al., 2022). In Section 4, we also\n",
      "brieﬂy compare LLaMA with instruction-tuned\n",
      "models such as OPT-IML (Iyer et al., 2022) and\n",
      "Flan-PaLM (Chung et al., 2022).We evaluate LLaMA on free-form generation\n",
      "tasks and multiple choice tasks. In the multiple\n",
      "choice tasks, the objective is to select the most\n",
      "appropriate completion among a set of given op-\n",
      "tions, based on a provided context. We select the\n",
      "completion with the highest likelihood given the\n",
      "provided context. We follow Gao et al. (2021)\n",
      "and use the likelihood normalized by the number\n",
      "of characters in the completion, except for certain\n",
      "datasets (OpenBookQA, BoolQ), for which we fol-\n",
      "low Brown et al. (2020), and select a completion\n",
      "based on the likelihood normalized by the likeli-\n",
      "hood of the completion given “Answer:” as context:\n",
      "P(completion|context )/P(completion|“Answer :”).\n",
      "0-shot 1-shot 5-shot 64-shot\n",
      "GPT-3 175B 14.6 23.0 - 29.9\n",
      "Gopher 280B 10.1 - 24.5 28.2\n",
      "Chinchilla 70B 16.6 - 31.5 35.5\n",
      "PaLM8B 8.4 10.6 - 14.6\n",
      "62B 18.1 26.5 - 27.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs_result[1].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " humanities, STEM and social sciences. We\n",
      "evaluate our models in the 5-shot setting, using the\n",
      "examples provided by the benchmark, and report\n",
      "results in Table 9. On this benchmark, we observe\n",
      "that the LLaMA-65B is behind both Chinchilla-\n",
      "70B and PaLM-540B by a few percent in average,\n",
      "and across most domains. A potential explanation\n",
      "is that we have used a limited amount of books\n",
      "and academic papers in our pre-training data, i.e.,\n",
      "ArXiv, Gutenberg and Books3, that sums up to only\n",
      "177GB, while these models were trained on up to\n",
      "2TB of books. This large quantity of books used\n",
      "by Gopher, Chinchilla and PaLM may also explain\n",
      "why Gopher outperforms GPT-3 on this benchmark,\n",
      "while it is comparable on other benchmarks.\n",
      "3.7 Evolution of performance during training\n",
      "During training, we tracked the performance of our\n",
      "models on a few question answering and common\n",
      "sense benchmarks, and report them in Figure 2.\n",
      "On most benchmarks, the performance improves\n",
      "steadily, and correlates with the training perplexity\n",
      "of the model (see Figure 1). The exceptions are\n",
      "SIQA and WinoGrande. Most notably, on SIQA,\n",
      "we observe a lot of variance in performance,\n"
     ]
    }
   ],
   "source": [
    "print(docs_result[2].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(docs_result[i].page_content)\n",
    "    print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
